{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Behavioral Cloning** \n",
    "\n",
    "## Writeup Template\n",
    "\n",
    "---\n",
    "\n",
    "**Behavioral Cloning Project**\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "* Use the simulator to collect data of good driving behavior\n",
    "* Build, a convolution neural network in Keras that predicts steering angles from images\n",
    "* Train and validate the model with a training and validation set\n",
    "* Test that the model successfully drives around track one without leaving the road\n",
    "* Summarize the results with a written report\n",
    "\n",
    "\n",
    "## Rubric Points\n",
    "Here I will consider the [rubric points](https://review.udacity.com/#!/rubrics/432/view) individually and describe how I addressed each point in my implementation.  \n",
    "\n",
    "---\n",
    "### Files Submitted & Code Quality\n",
    "\n",
    "#### 1. Submission includes all required files and can be used to run the simulator in autonomous mode\n",
    "\n",
    "My project includes the following files:\n",
    "* model.py: code to load data, build and train model\n",
    "* drive.py: code for driving the car in autonomous mode\n",
    "    * The only changed I've made to the orginal drive.py script was changing the `set_speed` parameter from `9` to `30`, mostly to speed up iteration speed.\n",
    "* model.h5: final model \n",
    "    * In addition, in the folder `nvidia_12_working/` there are the checkpoints of this model at each epoch during training.\n",
    "* video.mp4: video recording of the car driving track 1 for two laps, using `model.h5`\n",
    "* writeup_report.md: write up of this project. You are reading this.\n",
    "\n",
    "#### 2. Submission includes functional code\n",
    "Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing \n",
    "```sh\n",
    "python drive.py model.h5\n",
    "```\n",
    "\n",
    "#### 3. Submission code is usable and readable\n",
    "\n",
    "The `model.py` file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model, and it contains comments to explain how the code works.\n",
    "\n",
    "To run `model.py`, use the following command line arguments:\n",
    "```sh\n",
    "python model.py \\\n",
    "--model=nvidia \\\n",
    "--model_dir=12 \\\n",
    "--epochs=10 \\\n",
    "--data_dirs=\"../beta_simulator_mac/data/center_driving_1/driving_log.csv\" \\\n",
    "--data_dirs=\"../beta_simulator_mac/data/center_driving_2/driving_log.csv\" \\\n",
    "--data_dirs=\"../beta_simulator_mac/data/center_driving_reverse_1/driving_log.csv\" \\\n",
    "--data_dirs=\"../beta_simulator_mac/data/curves_1/driving_log.csv\"\n",
    "```\n",
    "\n",
    "`model`: one of \"vgg\" or \"nvidia\", the two models implemented.\n",
    "\n",
    "`model_dir`: specific a name for the directory to store the trained model.\n",
    "\n",
    "`epochs`: number of epochs to train the model.\n",
    "\n",
    "`data_dirs`: a list of `driving_log.csv` files that we want to train the model on; can pass this argument multiple times to train on multiple files.\n",
    "\n",
    "\n",
    "### Model Architecture and Training Strategy\n",
    "\n",
    "#### 1. An appropriate model architecture has been employed\n",
    "\n",
    "My (final, best performaning) model is based on this [NVIDIA convolutional neutral network model](https://devblogs.nvidia.com/deep-learning-self-driving-cars/).  \n",
    "\n",
    "Here's a summary of the model architecture:\n",
    "\n",
    "```\n",
    "Model: \"sequential_1\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "lambda_1 (Lambda)            (None, 160, 320, 3)       0         \n",
    "_________________________________________________________________\n",
    "cropping2d_1 (Cropping2D)    (None, 90, 320, 3)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_1 (Conv2D)            (None, 43, 158, 24)       1824      \n",
    "_________________________________________________________________\n",
    "conv2d_2 (Conv2D)            (None, 20, 77, 36)        21636     \n",
    "_________________________________________________________________\n",
    "dropout_1 (Dropout)          (None, 20, 77, 36)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_3 (Conv2D)            (None, 8, 37, 48)         43248     \n",
    "_________________________________________________________________\n",
    "conv2d_4 (Conv2D)            (None, 6, 35, 64)         27712     \n",
    "_________________________________________________________________\n",
    "conv2d_5 (Conv2D)            (None, 4, 33, 64)         36928     \n",
    "_________________________________________________________________\n",
    "dropout_2 (Dropout)          (None, 4, 33, 64)         0         \n",
    "_________________________________________________________________\n",
    "flatten_1 (Flatten)          (None, 8448)              0         \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 128)               1081472   \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 64)                8256      \n",
    "_________________________________________________________________\n",
    "dense_3 (Dense)              (None, 32)                2080      \n",
    "_________________________________________________________________\n",
    "dense_4 (Dense)              (None, 1)                 33        \n",
    "=================================================================\n",
    "Total params: 1,223,189\n",
    "Trainable params: 1,223,189\n",
    "Non-trainable params: 0\n",
    "```\n",
    "\n",
    "Additionally, I've also implemented and trained a model with VGG architecture. However, despite being a much larger model (about 10x the number of parameters of the NVIDIA model), this model performs significantly worse. It also takes much longer to train. My hypotheses are that 1) VGG is prone to overfitting when applied to this problem as the input image space is relatively homogenous, and 2) the extra pooling and dropout layers added too much noise and made it difficult for gradients to backpropogate through the network.  \n",
    "\n",
    "#### 2. Attempts to reduce overfitting in the model\n",
    "\n",
    "There are three main techniques applied to prevent overfitting:\n",
    "* Two Dropout layers\n",
    "* Early stopping in training\n",
    "* Attempts are made to collect a comprehensive and diverse set of training examples (more on this below).\n",
    "\n",
    "\n",
    "#### 3. Model parameter tuning\n",
    "\n",
    "The model used an adam optimizer, so the learning rate was not tuned manually.\n",
    "\n",
    "Other parameters of the model were tuned manually, by looking at the driving performance of resulting models.\n",
    "\n",
    "\n",
    "#### 4. Appropriate training data\n",
    "\n",
    "I generated training data from the following simulator runs on track 1:\n",
    "* Three laps of my \"best effort\" driving\n",
    "* Two laps of my \"best effort\" driving, in reverse direction\n",
    "* Additional runs for a couple of sharp that the model had trouble navigating initally\n",
    "\n",
    "I also used the following techniques to augment training data:\n",
    "* Flipping images and steering angles\n",
    "* Using images from left and right cameras\n",
    "    * This turned out to be particularly effective in teaching the model how to recover when its trajectory deviates from the center of the lane.\n",
    "    * I set the correction angle to be 0.15, based on some manual tuning.\n",
    "\n",
    "\n",
    "\n",
    "### Model Architecture and Training Strategy\n",
    "\n",
    "#### 1. Solution Design Approach\n",
    "\n",
    "I first developed the data input / model output modules, using a very simple model as a placeholder, in order to ensure that the pipeline is functional from end to end. This model performed miserably but that is as expected.\n",
    "\n",
    "I then beefed up my model by going to the VGG architecture. As discussed above, this appears to have overshot the target as the model is very slow to train, and more importantly the trained model performs badly on the track. The car would frequently go off track, fall into water, etc.\n",
    "\n",
    "Therefore I decided to use a simpler mode, i.e. the NVIDIA model introduced in the lectures. This model has done wonders. Within a few epochs of training the car was able to navigate large portions of the track. \n",
    "\n",
    "I then augmented the training data by leveraging images from left and right cameras - again as discussed above, this proved to be a critical step in teaching the car how to recover when it begins to go off track. After this the car was above to drive around track 1 smoothly.\n",
    "\n",
    "I added a few dropout layers to the original NVIDIA model to prevent overfitting. This did not have a noticable impact on driving performance on track 1, but (ostensibly) may make the model more generalizable to other scenarios.\n",
    "\n",
    "\n",
    "#### 2. Final Model Architecture\n",
    "\n",
    "See the \"1. An appropriate model architecture has been employed\" section above for a detailed description of the model architecture.\n",
    "\n",
    "#### 3. Creation of the Training Set & Training Process\n",
    "\n",
    "For training data creation see section \" 4. Appropriate training data\" above.\n",
    "\n",
    "I applied early stopping in training, and set the `restore_best_weights` parameter to `True` so that Keras automatically updates the final model weight to that from the best performing epoch.\n",
    "\n",
    "One interesting thing I observed is that below a certain threshold, validation loss does not correlate strongly with driving performance on the tracks. More specifically, the model from some epoch could a higher validation loss than one from another epoch, nonetheless its actual driving behavior on the track appears to be smoother, and more natural to how a human would drive the vehicle. My hypothesis is that, unlike e.g. image classification, there are multiple ways to control a car so that it does the right thing on the tracks. Turning right one fraction of a second sooner or later probably does not matter all that much. Therefore in this case the loss function functions more as a proxy metric, rather than the true task-success objective function \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 160, 320, 3)       0         \n",
      "_________________________________________________________________\n",
      "cropping2d_1 (Cropping2D)    (None, 90, 320, 3)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 43, 158, 24)       1824      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 77, 36)        21636     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20, 77, 36)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 37, 48)         43248     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 6, 35, 64)         27712     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 33, 64)         36928     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 33, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8448)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1081472   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,223,189\n",
      "Trainable params: 1,223,189\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
